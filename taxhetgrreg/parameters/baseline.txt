!This file contains parameters used in program taxhetgr ('regression' version).

!To run the program with a parameter in folder parameters, use command line
!argument. Example: taxhetgr baseline.txt (the suffix is not necessary)
!
!Lines beginning with '%' or '!' are ignored, trailing comments are also supported.
!
!Note on logical variables: they don't need to be in .true./.false. format,
!other values are supported (1/0, T/F).
!
!Note on matrices: row vectors should start on the same line as equality
!sign. When there are more rows then columns the data should start on the next
!row. No separators are needed except for spaces. This is not pretty, I should
!perhaps do something about this later on.
!__________________________________________________________________________

!___________Parameters governing what the program should do__________________

!Runtime_limit is the maximum runtime in hours. If stopping rule is not satisfied before
!the runtime limit is reached the program stops VFI iteration, saves the current value function,
!and computes and saves simulated series. 
runtime_limit = 6.0

!If search_for_bounds == .true., the program will cycle through various
!possible combinations of bounds for grid and look for such
!combinations that there is a feasible choice at every point in the grid
!These combinations will then be saved into a file. The program will then terminate.
search_for_bounds = .false.


!If sim_only == .true., then we assume that the value function for
!periods t=1,2,... has been solved, and the program proceeds with solving
!the problem for period 0 and simulating the series. The program will load
!a value function even if it comes from a different problem (or the number
!of grid points is different) using interpolation.
sim_only = .false.
input_folder = baseline_16_01_17_170801


!If load_initial_guess == .true., initial guess of value function will be loaded from
!the folder input_folder. Interpolation is used if the grid in input_folder
!is different than the grid in the current instance of the programme (for example
!if the number of grid points is different). If the policy function is present
!(saved if save_C_pol == .true.), then it is used as initial guess (no interpolation
!is done, it is assumed that the grid is the same in this case - the reason is that
!we would either have to do the interpolation on 1 image (slow) or we would have to
!pass the whole policy function to all images (consumes too much memory)).
!
!Also, if we are loading the initial guess we proceed directly into the last
!stage of CTS algorithm (no coarse grid).
load_initial_guess = .false.

!If save_C_guess == .true., the initial guess in maximization step in value function is saved
!every time the value function is saved. This is useful mostly when there is a time limit on a supercomputer
!which is insufficient to find a sufficiently accurate solution, and we want to continue
!where we left off (without having to find initial feasible guess which is very time-consuming
!for large grids, and avoiding some issues cause by non-convexity of feasible set).
!We need to have set the variable runtime_limit to account for this.
!
save_C_guess = .false.

!If disc_share_fit = > 0.0, 'outliers' are discarded before fitting the parameters in
!approximation of the value function. The outliers are discarded at every image (processor)
!separately. The 'outliers' are the points with the largest value at every
!image. This is a very primitive robust regression technique but it is very fast
!compared to more sophisticated methods. The number of discarded points is rounded below (floor).
!(this parameter is distinct from disc_share which is related to discarding solution paths)

disc_share_fit = 0.01

!N_sim is the number of simulated samples which are then used to compute
!statistics of interest. For every simulation a shock series is drawn and,
!if perm_eps>0, the initial asset holdings are permutated by random number
!from interval (b_init - eps, b_init + eps). This allows us to get results
!robust to initial conditions choice.
N_sim = 1

!perm_eps is the epsilon in permutation of initial asset holdings in simulations.
perm_eps = 0.00

!disc_share is the share of discounted simulations. These are the observations
!most extreme with respect to MU-adjusted asset holdings. The motivations
!is the fact that numerical optimizations sometimes yields large errors.
disc_share = 0.00

!reg_info tells the program what information related to regression approximation
!of the value function should be displayed at every iteration of VFI algorithm.
!value 0 means no extra information, 1 means R^2 (and adjusted R^2), 2 means
!t-statistics and p-values for individual explanatory variables (functions
!of states). Values other than 1 lead to additional consumption of memory
!and a bit of extra computation on one image (while other images are idle),
!so if either is an issue, this might be dropped.
reg_info = 1

!If grids_type = 1, equispaced grids will be used in VFI. If this is 2,
!the grids will be denser around the middle, with distance between grid points
!increasing quadratically as we move towards grid boundaries in either direction.
!This may be particularly useful in case of regression approximation, where we will
!in effect put more weight on approximating values closer to middle of grid
!(which should be the deterministic steady state if there is one).
!This does not apply to grid for productivity which is treated separately.
grids_type = 1

!V_fit_alg decides which algorithm is used to solve for the coefficients of 
!approximation of value function. Value 1 corresponds to using a direct solution
!method (such as SVD of the matrix X or the normal equations - this further
!depends on the value of parameter use_NE). Value 2 corresponds to gradient descent.
!This should almost always be used unless the number of explanatory variables is
!rather small, as the direct methods scale much worse and the way they
!are implemented here they can exhaust the memory.
V_fit_alg = 2


%__________________Parameters related to V_fit_alg = 1 (direct solution of OLS)________________
!IF V_fit_alg = 1, the program uses parameters use_NE, SVD_disc, and possibly rr_delta (if use_NE = .true.
!which is not recommended).

!if use_NE ==.true., the program uses normal equations of the OLS problem
!to approximate value function (one of several solution methods is available,
!and which one is used is determined by a hard-coded parameter inv_method in subroutine
!reg_M_prep in module mod_taxhetgr.f90). If this is .false., which is the
!recommended value, the program uses singular value decomposition to solve
!the system y = x*beta. In this case the t-statistics are not reported even
!if reg_info == 2, as the inverse of trans(X)X is not computed.
use_NE = .false.

!SVD_disc is the threshold value in singular value decomposition solution of OLS
!(all reciprocals of singular values lower than this threshold are discarded). This threshold
!should be a small non-negative number. For SVD_disc = 0.0 nothing is discarded.
!For guide as to how to choose this, check Chapter 15 of Numerical recipes in Fortran 77,
!second edition.
SVD_disc = 0.00001

!rr_delta gives the delta in ridge regression formula. Value 0.0 results in standard OLS,
!positive values yield adjustment of the trans(X)X matrix before inversion (a delta multiple
!of unit matrix is added), which could help in case of near-perfect multicollinearity. This
!is not used if use_NE = .false.
rr_delta = 0.0

%_________Parameters related to gradient descent_______________________________
!If V_fit_alg = 2, a version of gradient descent is used to solve for the coefficients
!in approximation of value function. In this case the program uses parameters GD_reg_lambda
!(lambda in regularization term), GD_max_iter (maximum number of iterations in gradient
!descent - which is run after every iteration of VFI algorithm), and more to be implemented:
!stopping criterion (based on change in cost, or change in pars?), then GD_select - if true we will
!run the GD in the first stage of VFI for lots of lambdas, will choose the one that minimizes
!the out of sample error in cross-validation subset of sample, and then reports the prediction error in
!test subset of the sample (in this case the sample will be split such that 0.6 will be the training set,
!0.2 in both the cross-validation and test sub samples). 

!IF GD_use_FS == .true., the explanatory variables (features) in approximating value
!functions are scaled, which means that the scaling functions are used. This accelerates
!convergence of gradient descent massively (but slows down the actual maximization
!step in VFI algorithm a bit). If this parameter is true then feature scaling is used irrespective
!of solution method (and in general it should be avoided if direct solution methods are
!used to get coefficients in approximation of V).
GD_use_FS = .true.

GD_reg_lambda = 1.0 !Lambda in regularization term (the GD is perhaps a bit misleading because
!this is a property of the cost function so this lambda is used even if other minimization approach
!than pure gradient descent is used).

GD_max_iter = 3000 !maximum number of iterations in gradient descent algorithm. This needs to be adjusted
!upwards if we-re using mini-batch version of the algorithm (i.e., not the whole sample but a combination
!of gradient descent and stochastic gradient descent).

GD_alpha = 10.0
!learning rate in the GD algorithm (later on can implement automatic setting). This is the starting
!value and is adjusted downwards if overshooting happens.

GD_stop_crit = 0.000001 
!Stopping criterion for gradient descent (we'll consider the algorithm to have converged if the
!absolute relative deviation is less than this)
!If the value is negative stopping rule is not checked (useful for debugging)

!The following parameters are related to mini-batch gradient descent. This is a combination of
!stochastic gradient descent and batch gradient descent (terminology from Machine Learning
!lectures of A Nguyen), in which not the whole sample is used but only a share. Then a step of
!this GD is much less costly in computation time, but is not guaranteed to be in the right direction.
!For this reason we want to use it only in the initial stages of VFI when the initial guess is bad,
!and then switch to large batches (hence large value of GD_MB_share for later iterations) or
!the full sample (batch gradient descent) to guarantee convergence.
!
!GD_MB_iter is the number of iterations for which the mini-batch gradient descent is used, after
!which the algorithm switches to the full batch. GD_MB_threshold lists values of iterations in
!VFI algorithm, and GD_MB_share contains the corresponding shares of the whole sample to be used.
!If the VFI_iter is less than GD_MB_threshold(k), then share GD_MB_share(k) of the full sample
!is used. If VFI_iter is greater than the maximum of GD_MB_threshold, then the full share is used.
!Also, no stopping rule checking is done before all the iterations GD_MB_iter is done (this could
!be improved later by perhaps adding a more relaxed stopping rule, after which we would switch
!to using the full sample).
GD_MB_iter = 100 
GD_MB_threshold = 10 20 40
GD_MB_share = 0.001 0.005 0.01
!What parameters work best needs to be checked experimentally. In particular, it is not clear
!how soon in VFI algorithm the solution from a previous iteration will be sufficiently good
!that using stochastic (mini-batch) gradient descent doesn't improve it (on average).
!
!Values in GD_MB_threshold need to be in ascending order (this is not checked by the program).



!__________Parameters for generating source code of approximating function____________
!If gen_mod_V_reg == .true., the program generates the module mod_V_reg.f90 containing
!approximating functions for V based on the parameters below.
!This module can then replace modules_mod_V_reg.f90 and program can be recompiled. This is useful
!for maximising efficiency.

!The parameters relevant to this are named with prefix GAF (Generate approximating function).
gen_mod_V_reg = .false.

!GAF_FS tells the program what feature scaling to use. This is extremely important
!if gradient descent is used (in experiments with 4 linear terms, convergence took about 10k
!iterations without feature scaling and 30 with feature scaling). It doesn't matter
!if direct solution methods are used. Values have the following meaning:
!0: no feature scaling
!1: normalization by range
!2: normalization by std. deviation
!3: normalization by range, demeaned
!4: normalization by std. deviation, demeaned
!All statistics used above are computed using the same set of grid points that the
!program uses to solve VFI (either random draws or points on a fixed grid), so if the 
!boundaries of grids are changed substantially, the feature scaling may no longer
!be appropriate, and new module should be generated.
GAF_FS = 3
!(Feature scaling is not used if the basis is 3 or 4 (Cheb pol.) as theey are
!all already in the proper range).

!GAF_basis is a (row) vector which contains one of defined values corresponding
!to particular basis of space of functions approximating the particular function.
!1 is tensor produc, 2 is complete polyn., 3 is tensor product of Cheb polynomials,
!4 is complete Cheb. pol (3 and 4 are just like 1 and 2 but with Cheb pol instead of
!standard pol). The names
!of the functions are hard-coded in include/taxhetgr_initialize.f90 (GAF_names).
GAF_basis = 3 2
!(in this case the second function is for the dynamic case).

!GAF_names (hardcoded in include/taxhetgr_initialize.f90) = [V_und_exp,V_und_exp_dyn]

!GAF_states (hardcoded in include/taxhetgr_initialize.f90) = [a1,a2,rho,t] 

!GAF_deg is a matrix where each row contains degree of polynomials to be used in approximating
!a particular function.
!Example for linear approximation (2nd function contains one extra state)
GAF_deg = 
5 5 5 0
1 1 1 1

!GAF_inter_deg is the highest degree of polynomials which appear in interaction terms.
!Value of -1 means no restriction (so the same order as in GAF_deg is used)
GAF_inter_deg = 
5 5 5 -1
-1 -1 -1 -1


!__________Parameters related to solution technique___________________

!If debugmode == .true., some additional things will be reported/plotted
debug_mode = .false.


!If static_environment == .true., a simpler version of the model is solved, in
!which the productivities of the agents are fixed at their initial values.

static_environment = .true.


!T_sim is the number of periods to be simulated. This includes period 0.
!for example, if T_sim = 5, periods t = 0,1,2,3,4 will be simulated.
T_sim = 30

!T_sim_max is the number of periods after which the growth stops

T_sim_max = 30

!Initial state of random number generator.
RNG_seed = 0


!__________Parameters which appear in the paper (environment)____________
!The number of players (I) and shock realizations (M) is hardcoded in 
!module mod_parameters_hard.

!The mass of agents in the population. The mass must sum to 1
mass = 0.5 0.5

%l_max is the maximum time that each agent can spend working.
l_max = 3.0 3.0

!Discount factor common to all agents.
beta = 0.95003

!theta_0: initial productivities
theta_0 = 3.6 1.0

!xi_theta is the vector of growth rates of productivities (in accordance with notation in the paper)
xi_theta = 0.0117 0.00

!Trend government expenditure as share of maximum output
k_gov = 0.058333

!s_t is a Markov chain - multiplicative shock to government expenditure. S
!denotes the row vector of possible shock realizations and P denotes
!the transition matrix.


!Here are some examples of how the specification would look like
!1 state (deterministic case) - this should not be required
S=1.0
P=1.0

!just for testing purposes (bad calibration of shocks)
!S = 0.95 1.05
!P = 
!0.5 0.5 
!0.5 0.5

!S = 0.9 1.0 1.1
!P = 
!0.4 0.3 0.3
!0.3 0.4 0.3
!0.3 0.3 0.4

!Initial shock realization index
s_init_index = 1

!The following - borrowing limits were disabled in the program (but they still
!were not removed from the parameters module, so they need to be here).

!borrowing limits for the agents (exogenous). b_min is the lowest allowed
!asset holdings (debt if negative) as share of current productivity. b_max
!is the maximum allowed asset holdings. These can differ for the various
!agents.

b_min = -1.0 -1.0
b_max = 5.0 5.0

!The following parameters determine the government's borrowing constraint
!(if the government can borrow, it is negative).
B_min_kappa = 5.0


B_min_number = -10.0 %(in levels - should be negative if the government can borrow)
B_min_constant = .false. %binary variable, if this is .true. then a constant B_min_number is used for B_min.


b_init = 0.0 0.0

!Pareto weights
alpha = 1.0 1.0

!_________Utility function parameters_____________________________
!So far only one functional form is supported (logarithmic in consumption,
!CRRA in labour). The utility funtion itself and all functions that
!are implementations of formulae which depend on the functional form
!are in a module mod_utility. When functional form is switched, we 
!replace the module. This way is computationally efficient (don't have
!to branch the program so much) and easy to implement.
!
!Values of parameters which govern the utility function are given here:

gamma_par = 2.0
A_par = 1.0
B_par = 2.9155

!___________Parameters governing VFI details_____________________

!The following parameters determine the stopping rule (for value function
!iteration).
stopping_rule_quantile = 1.0
stopping_rule_threshold = 0.00001

VFI_glob_opt_iter = 0 

!VFI_max_iterations is the maximum number of iterations which will be
!performed in value function iteration.
VFI_max_iter = 150

!save_iter_multiple: The value function is saved every time the iteration number
!of VFI algorithm is a multiple of this number. If this is zero, the value function is
!never saved. It usually makes sense to set this to a really large number in which case
!the value function is saved only at the end (after convergence criteria have been
!satisfied or if runtime runs out).
!
!If this is not zero, the policy function will also be saved at the same time (so we
!can resume VFI without looking for initial guess again, which takes a lot of time on
!large grids, and also leads to possible issues due to non-convexity of feasible set,
!so that we may have issues with convergence). If Howard algorithm is used, then
!The value and policy functions are saved only at the last iteration.
save_iter_multiple = 1000

!The following parameter is not used anymore (remnant of a previous version)
assume_interior_optimality = .false.

!_____Parameters governing looking for feasible choice (LFFC)______________
!These parameters are used wherever we look for a feasible choice, be it in 
!looking for feasible choice for a given grid, or in looking for a feasible grid.
!LFFC_min_share,LFFC_max_share are the minimum and maximum share of maximum feasible
!consumption which are used in the search. Particularly where l_max is large and
!almost never binding, it makes sense to restrict the maixmum share to something
!substantially lower than 1, as it increases the chance of finding feasible choices.
LFFC_min_share = 0.01
LFFC_max_share = 0.99

!LFFC_max_ind is the maximum index in search for feasible choice for every variable.
!In the case of M=1, this will be the actual number of choices tried. In case M>1,
!it would be LFFC_max_share**(2M-1), but the actual number is lower because the
!choices where share of consumption in a state sums to more than 1 is ruled out
!at an early stage of the algorithm. The state M consumption of agent I is treated
!separately. The higher this variable the better chance of finding feasible choices
!but it should be noted that this adds a lot of loops quite deep in the program so
!we can't raise this too high.
LFFC_max_ind = 30

!If interior_solution_only is .true., then (at every grid point) in the
!maximization procedure, we only consider points for which none of the
!borrowing constraints binds. This speeds up the computation but we could
!get a slightly wrong solution. This is not recommended - it could lead to
!a situation in which we have no feasible choices at the grid points (once
!we exclude this). It is preferable to use assume_interior_optimality = 1,
!which speeds up the computation less, but if a feasible choice in interior
!solutions is not found, it stills looks for feasible choice in corner
!solutions.
interior_solution_only = .true.

!If one_fc_suff is .true., then once the algorithm finds a feasible choice for
!any value of what_binds (starting with no constraints binding), it will stop
!looking for other feasible choices. This will speed up computation massively and
!hopefully it should affect the results less than interior_solution_only.
one_fc_suff = .true.

!_____Coarse to Smooth acceleration algorithm (CTS algorithm)______________

!If CTS_split_times = k, the number of all the gridpoints given in parameter
!file is divided by two k times, and the value function iteration (VFI) is
!performed on this coarse grid. Then a smoother grid is constructed, in
!which the number of grid points is (1/2)^(k-1), and the value function is
!solved on this smoother grid, using the value function obtained on the
!coarser grid as initial guess. This is done until we get to a grid which
!has the number of grid points as given in the parameter file. For example,
!if CTS_split_times = 1, we split the grid once, and if CTS_split_times =
!0, no CTS acceleration happens and VFI is solved on the grid defined by
!parameters in this file.
CTS_split_times = 0


!The number of grid points for a variable can never be lower than
!CTS_gridpoints_floor. If the value of CTS_VFI_times implies that this would
!happen, the number of grid points for the variable in question is set to
!CTS_gridpoints_floor. The motivation is that if we have less than this,
!the resulting approximation is probably quite useless as initial guess in
!the following steps of CTS algorithm). This floor is particularly useful
!when we have substantially different number of grid points for different
!state variables.
CTS_gridpoints_floor = 5

CTS_C_interp = .true.
!If CTS_C_interp = .true., then the policy function obtained in the previous
!stage of CTS algorithm is used (by interpolation) to get an initial guess
!of consumption in the maximization at all grid points (in first iteration
!of VFI algoritm). .true. is the recommended option because it saves time
!and helps stability. 

!_____Parameters governing grid_________________________________________
N_a = 50
%N_a is the number of gridpoints to be used for (utility adjusted) asset
%holdings. 

N_rho = 50
%N_rho is the number of gridpoints for the variable rho, which is the state
%variable - ratio of last-period marginal utilities. It is used only when I
%=2

rho_min = 0.18 %0 should be avoided, could lead to errors
rho_max = 0.3

%backup
%rho_min = 0.25 %0 should be avoided, could lead to errors
%rho_max = 0.75

N_t = 10
%N_t is the number of gridpoints for the state space of the 'time' variable
%which governs the deterministic components of time trends
%(such as productivities).


a_min = -3.0 -3.0
a_max = 3.0 3.0


VFI_interpolation_mode = 1
!VFI_interpolation mode is the mode of interpolation used for evaluating
!value function off the grid. It is an integer where various values
!correspond to certain modes of interpolaton.
!
!1 linear interpolation
!2,3,... so far not implmeneted (will have nearest neighbour and spline?)


!_____Searching for bounds________________________________________________
!If search_for_bounds == .true., the program will search
!for bounds a_min,a_max,rho_min,rho_max, such that there is
!a feasible point at every grid point. Feasibility in this sense
!contains the constraints on next-period 'artificial' state variables 
!being in the grid.


!The bounds rho_min and rho_max during the search are such that
!rho_search_mid is the midpoint of the interval for rho. The grid is
!gradually resized (from smallest to largest). rho_search_max_diff is the
!maximum difference between a corner of the interval and the mid point
!rho_search_mid (so that the maximum length of the interval is
!2*rho_search_max_diff).
!rho must be positive. rho_search floor is the lowest rho_min used in
!finding the feasible solution.

rho_search_mid = 0.9
rho_search_max_diff = 0.4
rho_search_floor = 0.01


!a_search min and a_search max are the widest grids a_min and a_max used in
!the search algorithm. The algorithm starts in the middle and gradually
!increases the size of these grids until these bounds are reached in
!N_search_a steps.
a_search_min = -5.0 -5.0
a_search_max = 5.0 5.0


!If search_cycle_over_both = .true., then there are two separate cycles (grid
!for a1, and grid for a2). If this is .false., there is only one cycle (the
!size of both grids increasing simultaneously). This reduces runtime
!substantially but we may not find some feasible grids - if finding feasible
!grids proves to be a significant problem later on, I should generalize this.
!search_cycle_over_both = .false.


!Number of points for each of the variables to perform the search.
N_search_a = 10
N_search_rho = 20


!If check_corners_only = .true, then for the purpose of checking feasibility, the number of
!all grid points will be limited to 2. This effectively means that only
!corner values will be checked. This speeds up the computation massively
!but sometimes, there could be violations 'close to the corner' even if
!there are no violations at the corner. More often than not, it seems
!That if feasibility is not an issue at the corner of the grid, it is also
!not a problem at other points (corners of the grid are in a sense extreme).
check_corners_only = .true.

!If stop_when_found == .true., the program stops once a feasible grid
!was found. ('Feasible grid' is explained in the appendix - it is simply a grid
!on which we always have a feasible choice, i.e., a choice s.t. the next-period
!points lie in the grid).
stop_when_found = .false.

!___________________________________________________
!The following parameters govern Howard acceleration algorithm.
!The algorithm works as follows. First a criterion is chosen (depending on crit_How).
!This could be for example the maximum absolute relative deviation in Value function (b/w iterations).
!Then, if this criterion is lower than any of elements of a_How, we find the lowest of these
!elements for which criterion is lower (call it i), and then a correspondinng b_How. The maximization step
!is then skipped b_How(i) times. If the criterion is lower than a threshold c_How, then no more skipping
!will be performed.
!This setup allows us to skip more maximization in later stages of the algorithm when the changes
!in policy (and V function) should be lower. Then, once criterion is really low, we no longer do these changes to
!avoid overshooting, and hopefully facilitate convergence.
!
!The dimension of a_How and b_How is hardcoded in mod_parameters_hard.

crit_How = 0 !1 works fine

!(1 corresponds to maximum absolute relative deviation in V (MARD), 2 to average relative deviation in V (AARD),
!3 to MARD in policy function, 4 to AARD in policy function. Value crit_How = 0 means that Howard
!acceleration algorithm is not used.

!Should have a_How in descending order, b_How in ascending (not necessarily but it makes most sense
!and the algorithm is implemented under this assumption)
a_How = 0.5 0.1 0.05
b_How = 20 10 5
c_How = 0.001

!End of parameter file (this should be kept here to avoid potential eof problems)
