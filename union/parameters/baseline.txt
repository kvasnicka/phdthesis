!This file contains parameters used in program union. File baseline.txt contains all the
!parameter choices with explanations.
!
!To run the program with a parameter in folder parameters, use command line
!argument. Example: union baseline.txt (the suffix '.txt' is not necessary)
!
!Lines beginning with '%' or '!' are ignored, trailing comments are also supported.
!
!Note on logical variables: they don't need to be in .true./.false. format,
!other values are supported (1/0, T/F).
!
!Note on matrices: row vectors should start on the same line as equality
!sign. When there are more rows then columns the data should start on the next
!row. No separators are needed except for spaces. This is not pretty, I should
!perhaps do something about this later on.
!__________________________________________________________________________

!___________General parameters telling the program what to do__________________

!Runtime_limit is the maximum runtime in hours. If the stopping rule is not satisfied before
!the runtime limit is reached, the program stops VFI iteration, saves the current value function,
!and computes and saves simulated series. This is useful particularly on HPC because there
!are runtime limits, and if we are approaching the runtime limit, we want to
!save the current value function and generate simulations (get the best solution we could
!in the runtime limit). If we think that more iterations will be needed we should also
!save the policy function (finding a good initial guess is costly).
runtime_limit = 11.5

!Input folder is the name of the subfolder of 'results' folder, from which
!the program loads value function (and possibly policy function) - whether
!this is done depends on the values of parameters sim_only and load_initial_guess,
!which are explained further below.
input_folder = baseline_17_07_19_130159

!If sim_only == .true., then we assume that the value function for
!periods t=1,2,... has been solved, and the program proceeds with solving
!the problem for period 0 and simulating the model. The program will load
!a value function even if it comes from a different problem (or the number
!of grid points is different) using interpolation, so we need to be careful
!about not loading a wrong value function because then we would get nonsensical
!results.
sim_only = .false.

!If load_initial_guess == .true., initial guess of value function will be loaded from
!the folder input_folder. Interpolation is used if the grid in input_folder
!is different than the grid in the current problem. If the policy function is present
!(saved if save_pol == .true.), then it is used as initial guess (no interpolation
!is done, it is assumed that the grid is the same in this case - the reason is that
!we would either have to do the interpolation on 1 image (slow) or we would have to
!pass the whole policy function to all images (consumes too much memory)).
!
!This function is particularly useful in the case of dealing with runtime limits
!on HPC (on Cambridge HPC this is usually at most 12 hours or 36 hours for
!premium users).
!Sometimes the VFI does not converge before the runtime limit, in which
!case we can just continue with more VFI iterations where we left off
!(it is advisible to also save the policy function - but somethimes this may 
!be infeasible because it takes 5 times as much memory as the value function
!if there are 2 countries and 2 shock realizations).
!
!Also, if we are loading the initial guess we proceed directly into the last
!stage of CTS algorithm (no coarse grid) - i.e., CTS_split_times is set to 0.
load_initial_guess = .false.

!If save_pol == .true., the policy function is also saved every time that the value is
!saved. It takes time to save, and it takes up a lot of space.
save_pol = .true.

!N_sim is the number of simulated samples which are used to compute
!statistics of interest. For every simulation a shock series is drawn and,
!if perm_eps>0, the initial government debt is set randomly
!from interval (b_init - eps, b_init + eps), and similarly for the external debt.
!This is useful mainly because the inaccuracies accumulate for very long
!series, so to compute statistics, it is better to draw many samples which are relatively
!short (maybe 50 years).
N_sim = 1

!T_sim is the number of periods to be simulated. This includes period 0.
!for example, if T_sim = 5, periods t = 0,1,2,3,4 will be simulated.
T_sim = 150

!perm_eps is the epsilon in permutation of initial asset holdings in simulations.
!(the same for each of b_1,b_2,b_ex).
perm_eps = 0.00

!disc_share is the share of discounted observations. These are the observations
!most extreme with respect to MU-adjusted asset holdings. The motivations
!is the fact that numerical optimizations sometimes yields large errors
!which accumulate over time, and this criterion discards the outliers with the
!worst cumulative errors. In the future we should probably switch to
!discarding outliers based on EE residuals (I am not sure how much of
!a differerence this will make but it is a more natural approach). In any
!case, this parameter should be used very carefully.
disc_share = 0.0

!If debugmode == .true., some additional information will be reported
debug_mode = .false.

!IF NDA is true, then no deallocations of variables are made on in the last stage
!of CTS algorithm. This can potentially lead to running out of memory and it should be
!used very carefully. The reason for including is that sometimes if run the program on
!multiple nodes on HPC, then deallocating some large variables introduces errors
!elsewhere in the program in transferring data between images. It is some weird
!but in Intel Fortran Compiler at the time when I was writing the program (or perhaps
!it is an error in configuration of Cambridge HPC). The default value of NDA is false.
!It should be true every time when we want to continue a large scale VFI on multiple nodes.
NDA = .false.

!Initial state of random number generator (there is almost never a reason to change
!this).
RNG_seed = -1

!If option gen_transfers_ss is .true. (default value is .false.), then the program does
!not solve the model. Instead it solves a version of the model in which the social planner
!is allowed to use transfers between governments, so there is effectively only one
!government with one budget constraint. It is done only if the aggregate expenditures
!are constant across states, otherwise it is of little interest to generate some SS like
!this. Also currently only the case of equal size of countrie (mass(1) = mass(2)) is supported.
!Also this works only if initial assets are zero (because the SS computation assumes that
!so we can get an analytical solution for SS). This can be generalized in the future.
gen_transfers_ss = .false.

!__________Parameters which appear in the paper (environment)____________
!The number of countries (I) and shock realizations (M) is hardcoded in 
!module mod_parameters_hard. So far the program works for 2 countries
!and arbitrary number of shock realizations.

!The mass of countries. It has to sum to 1 (otherwise the program will
!exit).
mass = 0.5 0.5

!l_max is the maximum time that HH in each country can spend working.
!Setting a relatively high number here (such as 3 or even more) can help
!with stability (Shin made this point), almost never reached in practice.
l_max = 3.0 3.0

!Discount factor common to all HHs.
beta = 0.95003

!theta_0: initial productivities of the two countries. In this model there
!is no growth so productivity in every period is equal to the initial
!productivity (we can introduce productivity shocks at some point)
theta_0 = 1.0 1.0

!The following parameters govern the shock process. So far there are shocks to government
!expenditures only.

!Vector g_share gives average per-capita government expenditure in each country as share of maximum
!per-capita output in that country (which depends on max. labour supply, which may not be 1).
!For example, if productivity in both countries and labour supply in both countries is 1,
!then the maximum per-capita output in a country is 3. gov_share = (0.1,.0.05), then in country 1
!the average goverment expenditure (per capita) is 0.3, and it is 0.15 in country 2.
!So g_avg = (0.3,0.15).The actual per-capita expenditure is the average one multiplied by an appropriate element of matrix G_shock
!which is defined below (see the description there).
!
!Note: Because for stability reasons the maximum labour supply tends to be about 4 times as much as the
!actual labour supply, a value of around 0.05 leads to roughly 20% tax rate in a steady state with 0 assets.
!This depends on the utility function - we need to experiment with these.
g_share = 0.058333 0.058333

!s_t is a Markov chain - multiplicative shock to government expenditure.
!Each row m of G_shock gives the multiplicative shocks for both countries,
!if shock realization with index m occured.
!P is the transition matrix.

!If IID_shocks = .true., then shocks are i.i.d. In this case the first row
!of the transition matrix P is used as vector of probabilities of shocks realizations
!regardless of the current shocks realization. The value function will not depend
!on shock realization directly, and the number of grid points for shock
!realization in constructing value function will be 1.
IID_shocks = .true.

!Matrix G_shock contains multiplicative shock to government expenditure
!
!The actual government expenditure in country i, if shock realization is m will be g_avg(i)*G(m,i).
!(see above for how we get g_avg using parameter g_share). This is for convenience of calibration,
!so we don't have to change all elements of G if we want to change government expenditure relative to GDP,
!and allows a lot of flexibility (such as different countries having different gov. expenditure as share
!of GDP (in a deterministic steady state).

!If the matrix P or G_shock are of the wrong dimension, the program will crash!
!We need to be sure that it correponds to value of M_par set in mod_parameters_hard.f90.

!Example: Aggregate shocks
!This example corresponds to 5 percent drop/increase in government expenditure relative to average level
!in both countries at the same time (aggregate shock).
!G_shock =
!0.95 0.95
!1.05 1.05

!Example: Idiosyncratic shocks (aggregate expenditure constant if countries are identical)
G_shock =
0.95 1.05
1.05 0.95

!Degenerate example useful for testing: zero variance shocks - we should get a constant solution
!G_shock =
!1.00 1.00
!1.00 1.00

!The transition matrix
P = 
0.5 0.5
0.5 0.5


!P_sim is the transition matrix used in simulations for drawing the shocks.
!The expected returns etc. are still governed by P. This is only used if variable
!use_P_sim is true (the default option is false). This is useful for debugging. For example
!we can set P so that the shock realization is always equal to the initial condition,
!or that the shock realization always alternates.
use_P_sim = .false.
P_sim = 
0.0 1.0
1.0 0.0

!Initial shock realization index (-1 corresponds to randomization according to SS probabilities).
!The initial shock realization effectively introduces a small trend in government expenditures.
!In some situations it is better to randomize over initial shock realiation.
s_init_index = -1

!if load_shocks = .true., then shocks are loaded from folder results/'shocks_path'.
!For example, if shocks are in fils shocks.out in folder results/just put
!shocks_path = shocks.out.
!Also, we need to give shocks_load_N and shocks_load_T which must be the same
!which were used for generating shocks. If shocks_gen_only = .true., then the program generates shocks
!only (and saves them in the results folder as shocks.out).
load_shocks = .true.
shocks_path = shocks.out
shocks_T = 500
shocks_N = 10000
shocks_gen_only = .false.

!The following parameters (b_init,b_ex_init) determine the initial distribution of debt.
!We use the normalization b^2_1 = 0 (so that country 1 households do not hold
!debt of country 2).

!b_init = 0.1*ones(1,I);
!b_init are initial (per-capita) asset holdings in both countries (row vector,
!i-th element corresponds to country i)
b_init = 0.0 0.0

!b_ex_init is the initial net external debt of country 1 (external debt of country 2
!is minus 1 multiple of this)
b_ex_init = 0.0

!Pareto weights
alpha = 1.0 1.0

!_________Utility function parameters_____________________________
!So far only one functional form is supported (logarithmic in consumption,
!CRRA in labour). The utility funtion itself and all functions that
!are implementations of formulae which depend on the functional form
!are in a module mod_utility. When functional form is switched, we 
!replace the module. This way is computationally efficient (don't have
!to branch the program so much) and easy to implement.
!
!Values of parameters which govern the utility function are given here:

!Log utility calibration
!gamma_par = 2.0
!A_par = 1.0 !redundant
!B_par = 2.9155
!sigma_par = 1.0

!For sigma2 this calibration works (we need to change B_par so that ss labour supply
!(and thus ss output) remains the same as with log preferences.
!gamma_par = 2.0
!A_par = 1.0
!B_par = 5.55324170484
!sigma_par = 2.0

!For sigma3 use the following instead
gamma_par = 2.0
A_par = 1.0 !redundant
B_par = 10.5776032473242
sigma_par = 3.0


!The utility function is 
!u(c,n) = (c^(1-sigma_par))/(1-sigma_par) - B_par*(n^(1+gamma_par))/(1+gamma_par))

!___________Parameters governing VFI details_____________________

!The following parameters determine the stopping rule (for value function
!iteration). If stopping rule quantile is 1.0, then the stopping rule is maximum
!absolute relative deviation. If it is less than one, it is a quantile stopping rule.
!I recommend to always use 1.0 (not because it inherently makes more sense
!but because the quantiles take long to compute with very large number of
!grid points).
stopping_rule_quantile = 1.0
stopping_rule_threshold = 0.001

!VFI_max_iterations is the maximum number of iterations which will be
!performed in value function iteration. If we are using Howard's acceleration
!algorithm, this is ignored, and instead we use only the runtime limit.
!The reason is that with Howard's algorithm the number of iterations to
!achieve a given accuracy of solution is much higher/
VFI_max_iter = 10

!save_iter_multiple: The value function is saved every time the iteration number
!in VFI algorithm is a multiple of this number. If this is zero, the value function is
!never saved. It usually makes sense to set this to a really large number in which case
!the value function is saved only at the end (after convergence criteria have been
!satisfied or if runtime runs out).
!
!If this is not zero, the policy function will also be saved at the same time (so we
!can resume VFI without looking for initial guess again, which takes a lot of time on
!large grids, and also leads to possible issues due to non-convexity of feasible set,
!so that we may have issues with convergence). If Howard algorithm is used, then
!The value and policy functions are saved only at the last iteration.
save_iter_multiple = 10000000

!_____Coarse to Smooth acceleration algorithm (CTS algorithm)______________

!If CTS_split_times = k, the number of all the grid points given in parameter
!file is divided by two k times, and the value function iteration (VFI) is
!performed on this coarse grid. Then a smoother grid is constructed, in
!which the number of grid points is (1/2)^(k-1), and the value function is
!solved on this smoother grid, using the value function obtained on the
!coarser grid as initial guess. This is done until we get to a grid which
!has the number of grid points as given in the parameter file. For example,
!if CTS_split_times = 1, we split the grid once, and if CTS_split_times =
!0, no CTS acceleration happens and VFI is solved on the grid defined by
!parameters in this file.
CTS_split_times = 0

!The number of grid points for a variable can never be lower than
!CTS_gridpoints_floor. If the value of CTS_VFI_times implies that this would
!happen, the number of grid points for the variable in question is set to
!CTS_gridpoints_floor. The reason is that if the number of grid points
!for any particular variable is very low, the resulting approximation
!is very poor, and is likely to be useless as initial guess in the
!next stage of CTS.
! This floor is particularly useful when we have substantially different
! number of grid points for different state variables.
CTS_gridpoints_floor = 5

!If CTS_C_interp = .true., then the policy function obtained in the previous
!stage of CTS algorithm is used (by interpolation) to get an initial guess
!of consumption in the maximization at all grid points (in first iteration
!of VFI algoritm). .true. is the recommended option because it saves time
!and helps stability. 
CTS_C_interp = .true.

!CTS_first_maxatt is the number of maximization attempts (permutations of initial guess)
!which is done in the first iteration of VFI algortihm, if we are in the first
!stage of CTS algorithm. Ideally we would like to try a lot of initial guesses so that
!we are not stuck in a bad local optimum every time we maximize - but this is not feasible
!due to curse of dimensionality. So we do it at least the first time in CTS algorithm,
!where the number of grid points is lower (and then we use interpolation).
!A reasonable value is perhaps around 50 (in the first stage of CTS, if CTS_split_times = 1
!there are 1/16 as many gridpoints, so with the value of 50, this will take about as much
!time as 3 iterations in the second stage of CTS (perhaps a bit less due to
!lower communication time).
!
!Warning: This needs to be an integer greater than or equal to 1.
!Also - if CTS algorithm is not used at all, then the number of attempts in the first
!iteration will be CTS_first_maxatt anyway. So one must be careful about this
!(if the number of GPS is great, then a value of CTS_first_maxatt > 1 will lead
!to slow execution).
CTS_first_maxatt = 10

!Note: CTS_first_maxatt is only used with certain probability at each grid point.
!if VFI_attmaxx_RND = 0, this is 1 at each grid point. Otherwise it can be
!smaller - it is the same probability that is used with VFI_attmax.
!(see there).

!If parameter always_perm_first_iter = .true., then the permutations driven by
!CTS_first_maxatt are always done in the first iteration of VFI, regardless of
!CTS algorithm stage. This is useful because sometimes we have a value function and we want
!to check whether we can further improve it by doing more permutations, and if we just
!load it then no permutations would be done - because the algorithm only does them when
!we are in the first stage of CTS, and it is not the only stage. Also it is useful in
!the case when we set CTS_split_times = 0, in which case the value of CTS_first_maxxatt
!would also be ignored and only 1 attempt would be made.
always_perm_first_iter = .false.

!NOTE: Also see parameter VFI_attmax.


!_____Parameters governing grid_________________________________________
!If grids_type = 1, equispaced grids will be used in VFI. If this is 2,
!the grids will be denser around the middle, with distance between grid points
!increasing quadratically as we move towards grid boundaries in either direction.
!Value 3 means grids denser around the middle (quadratic as in 2) only for rho.
!WARNING - grids type /= 1 actually makes the problem of discontinuities of derivatives
!worse and leads to worse stability - unless we are using a more sophisticated
!interpolation scheme than quadrilinear interpolation. It is strongly recommended
!to use grids_type = 1 (perhaps if I implement 4D spline in the future I can
!experiment with dense grids again). 
grids_type = 1

!N_a is the number of gridpoints to be used for (utility adjusted) asset
!holdings. I use the same number of gridpoints for all countries. In the future,
!if we are interested in calibrations with asymmetrical countries, in which
!one country's asset holdings are expected to fluctuate more, it might make sense to
!generalize this.
N_a = 5

!N_aex is the number of gridpoints for external debt of country 1 (not sure if
!there is much to be gained by allowing this to be different than N_a but it is not
!very costly to do this).
N_aex = 5

!N_rho is the number of gridpoints for the variable rho, which is the state
!variable - ratio of last-period marginal utilities.
N_rho = 5

!If rho_gr_symm = .true., then the grid for rho will be constructed in such
!way that:
!	(1) The number of grid points will be set to an odd number (rounding up)
!	(2) 1 will be on the grid
!	(3) The 'left' part of the grid is constructed as usual, covering interval [rho_min, 1]
!	(4) The right part of the grid is constructed such that for every point rho < 1 in the left part
!		of the grid, there is a corresponding point on the right part of the grid 1/rho > 1. The
!		rho_max (end point of the grid) will be set to 1/rho_min (rho_max in parameter file is ignored).		
!	(5) The 'left part of the grid is either equispaced or it can be denser closer to 1, depending
!		on grids_type setting (the same for other grids).
!
!This specification makes sense if countries are ex ante identical, and the number of grid points for rho
!is fairly low (so the inaccuracy caused by interpolation is in some sense symmetrical around the point
!where the agents' marginal utilities of consumption are equal).
rho_gr_symm = .true.

!rho_min and rho_max are boundaries for the gridpoint for rho. The
!appropriateness depends on utility functions, productivities, etc. So it
!will have to be adjusted experimentally. With symmetric countries, it should
!be symmetric around 1, so that if 0<r<1 is the lower bound, then 1/r is the
!upper bound. If rho_gr_symm = .true., then this is imposed by the program and
!value of rho_max is ignored.
rho_min = 0.8 %values very close to zero should be avoided, could lead to errors
rho_max = 1.25

!a_min and a_max are vector which contain the lower and upper bounds for government debt
!for both countries (marginal-utility adjusted).
a_min = -5.0 -5.0
a_max = 5.0 5.0

!aex_min and aex_max are the minimum and maximum external debt (MU-adjusted) of
!country 1.
aex_min = -5.0
aex_max = 5.0

!VFI_interpolation mode is the mode of interpolation used for evaluating
!value function off the grid. It is an integer where various values
!correspond to certain modes of interpolaton:
!	- 1: quadrilinear interpolation (this is a generalization of bilinear
!		interpolation). This is the fastest method, but it suffers from some issues,
!		mainly the discontinuity of derivatives. But it is also more robust than the
!		other methods in terms of convergence. Perhaps we can get a better accuracy
!		using a different interpolation scheme.
!	- 2: Shepard's interpolation (local). This interpolation is also not shape-preserving
!		but it uses more information than quadrilinear interp (it is a weighted average of points
!		on a grid of shep_n points in every direction, the weights are inversely related to distance
!		from the point at which we interpolate). Because it is local (it does not use all points
!		due to computational feasibility), it suffers from discontinuities of derivatives as the quadrilinear
!		interpolation. On the other hand, it uses more information (more points than the quadrilinear
!		interpolation, as long as shep_n > 2. It is also very easy to implement compared to
!		shape-preserving interpolations.
!	- 3: 4-D bspline interpolation. The order of spline is linear bspline by default, and it can be
!		changed by changing values of parameters bspline_k_a1,bspline_k_a2,bspline_k_rho,bspline_k_aex
!		(the value 2 which is default corresponds to linear spline). It is recommended to start off with
!		the linear case for all states, and then gradually increasing the orders (the linear case
!		stable, but higher orders sometimes lead to oscillations). The value bspline_N_nonlin gives the
!		program the minimum number of gridpoints below which only linear spline is used for the variable
!		in question. This is useful for particular in combination with CTS algorithm (if we use multiple stages
!		because divergence is particularly likely for nonlinear bsplines if the number of gridpoints is low and)
!		It is checked one variable at a time.
	
VFI_interpolation_mode = 1

bspline_k_a1 = 2
bspline_k_a2 = 2
bspline_k_rho = 2
bspline_k_aex = 2

!The following actually leads (in the current version) to bugs, probably related to pointers (V_old_pntr) - 
!there might be a typo somewhere related to allocation of variables). It is recommended to use a low value
!(such as 5) so it is never used in practice. It will be fixed in a future release.
bspline_N_nonlin = 5

!if bspline_check_V_convergence is true, then (before bspline coefficients are computed, which overwrites
!the original value function in the process) a copy is created, so we can compute the same convergence
!criteria as before. However, it is slightly wasteful, and usually we just want to keep iterating as long as
!we can - so we can just use policy function change as converge criterion instead, which is always
!computed.
bspline_check_V_convergence = .true.

!x_interpolation_mode is the same as VFI_interpolation mode but it is used in the case
!of policy function interpolation. This makes sense for example between stages of CTS
!algorithm, where the interpolated value of policy function from the coarse grid
!will probably be quite far from optimum regardless of the interpolation scheme, so it makes
!sense to use a simple one rather than waste time by something more complicated.
!The default value of this parameter is -1, in which case the same interpolation scheme
!is used as for the value function (so x_interpolation_mode is overwritten by VFI_interpolation_mode).
!The only exception is when VFI_interpolation mode is 3 (bspline) which is not supported for
!policy function interpolation right now, so instead x_interpolation_mode is set to 1 in this case
!(quadrilinear).
x_interpolation_mode = -1

!The following two parameters are used only if VFI_interpolation_mode = 2 (local Shepard's method).
!shep_n is the number of grid points in each dimensions that are used in the interpolation
!(suggested value - between 3 and 5)
shep_n = 4

!shep_floor is a number which is added to squared distance before inverting to compute
!the weight. The default value is 1.0E-12, but perhaps it is better to make it
!about 1.0E-30. Otherwise there is a little bit of error added every time
!the algorithm hits a grid point exactly.
shep_floor = 1.0E-12
!shep_floor = 1.0E-30

!IF shep_norm is true, we do not use distance, but distance as share of the
!overall range of state space for a variable, when calculating the weights. The
!intuition is as follows. If points A and B in state space are equally distant
!from the point x at which we interpolate, but point A is more distant in the
!dimension in which small changes in state result in large changes in value,
!(in our example this is rho), then intuitively point A is less 'informative' about
!the value at x than point B. If we normalize the distance by (rho_max - rho_min for
!rho, and a_max - a_min for other variables), we take this into account.
!It is not clear if this is better - I allow both options (default value should
!be shep_norm = .false.) - and we should choose whichever is better based on
!formal accuracy tests and simulation results. For details about the
!normalization, see the subroutine interp_V_shep in mod_union.f90.

shep_norm = .false.


!______Parameters related to Howard's algorithm__________________

!The algorithm works as follows. First a criterion is chosen (depending on crit_How).
!This could be for example the maximum absolute relative deviation in Value function (b/w iterations).
!Then, if this criterion is lower than any of elements of a_How, we find the lowest of these
!elements for which criterion is lower (call it i), and then a correspondinng b_How. The maximization step
!is then skipped b_How(i) times. If the criterion is lower than a threshold c_How, then no more skipping
!will be performed.
!This setup allows us to skip more maximization in later stages of the algorithm when the changes
!in policy (and V function) should be lower. Then, once criterion is really low, we stop skipping
!the maximization steps to avoid overshooting.
!
!The dimension of a_How and b_How is hardcoded in mod_parameters_hard.

crit_How = 0 !With a large number of grid points, 2 works well (outliers possible). 4 would also work.

!(1 corresponds to maximum absolute relative deviation in V (MARD), 2 to average relative deviation in V (AARD),
!3 to MARD in policy function, 4 to AARD in policy function. Value crit_How = 0 means that Howard
!acceleration algorithm is not used.

!Should have a_How in descending order, b_How in ascending (not necessarily but it makes most sense
!and the algorithm is implemented under this assumption)
a_How = 0.5 0.1 0.05
b_How = 10 20 10
c_How = 0.0001


!______Parameters related to accuracy & accuracy tests____________________________
!(the accuracy tests are not implemented in this version)
!acc_int is a real number (should be in (0.0,1.0) interval that tells the program
!what intervals to use for drawing points used in accuracy tests. The intervals 
!have the same mean as the ones used in solving the problem (a_min,a_max,rho_min,rho_max),
!and their length is acc_int multiple of the original length.
!The distribution used to draw sample is multivariate uniform over the reduced length intervals.
acc_int = 0.0

!acc_samples is the number of samples used to compute the accuracy test statistics
acc_samples = 1

!acc_method is the method used to compute accuracy tests. 0 corresponds to no accuracy
!test, 1 to Euler equations residuals (average over the interval).
acc_method = 1

!acc_attmax1 is the number of random initializations in the accuracy test in first period
!(should be fairly large), and acc_attmax2 is the number of initializations in period 
!(this does not need to be as large because period 1 solution should provide a good
!initial guess). Acc_maxiter is the maximum number of iterations in the optimization
!subroutine called to find maximum of current + continuation value at a gridpoint.
!The larger this is the higher the accuracy should be, but the longer it takes to solve.
acc_attmax1 = 10
acc_attmax2 = 10
acc_maxiter = 500

!Other parameters related to accuracy of solution (these might eventually be moved to LFFC section
!of the parameter file)
!LFFC_max_att is the maximum number of attempts (at every grid point) in looking for feasible choice.
!The search algorithm is initialized at a given point in state space and the more times we do it
!the more likely it is that we find a good initial guess.
LFFC_attmax = 10

!If LFFC_kappa > 0, then the difference in government expenditure between the two countries
!is taken into account in constructing the initial guess. This is particularly useful
!if shocks are purely idiosyncratic, with prely aggregate shocks it makes no difference.
!It should be a number between 0 and 1, with 1 corresponding to the benchmark of balanced
!budget constraint (so perhaps a value of about 0.7-0.8 might be reasonable but ultimately
!the best value of the parameter should be based on accuracy tests results).
LFFC_kappa = 0.3

!LFFC_maxiter is the maximum number of iterations in the optimization subroutine used in LFFC.
LFFC_maxiter = 100

!LFFC_neg_loss - if this is true, then stricly feasible points are rewarded. This might lead
!to slightly better guess, but also possible slight violations of some constraint. The reward
!is small relative to the harsh penalization for violation.
LFFC_neg_loss = .true.

!If LFFC_use_new_guess is true, the program uses a 'new' algorithm for constructing initial
!guess. This should be better in theory but in some cases it performs worse, so I allowed
!this option in the parameter file. .false. is the safer option perhaps, but more testing
!needs to be done. The default value (in case this parameter is not present is .false.).
LFFC_use_new_guess = .false.

!VFI_opt_maxiter is the maximum number of iterations in the optimization subroutine used at
!every iteration of VFI algorithm at every gridpoint. This parameter has the potential to
!increase the accuracy of the solution a lot but also the runtime can be prohibitively
!high if this is too high.
VFI_opt_maxiter = 200

!VFI_attmax is the number of permutations that are done in every iterations to the
!initial guess (which is the policy at the grid point from previous iterations). In some cases
!this is needed to break out of being stuck at a local optimum (this seems to be an
!issue particularly in the case of purely idiosyncratic shocks). However, this is very costly,
!neglecting communication time, setting this to 10 will mean that each iteration takes 10 times as much
!time. The default value is 1. An alternative is to use CTS_first_maxatt which does the same
!thing but only once in the first iteration of CTS and VFI algorithm. We can thus afford many attempts
!there (because the grid in the first stage of CTS only has roughly 1/16 total number of grid points),
!but it is not obvious if it is good enough to find a 'global' optimum once at the outset,
!or whether we need to check repeatedly whether we are at a local optimum. In an ideal
!world we would do a huge number of attempts every iteration but that is not feasible
!due to runtime.
VFI_attmax = 5

!VFI_perm is the multiplicative permutation which is applied to the initial guess
!(for example if this is 0.05 which is the default value then the permutation is
!a multiplicative shock uniformly drawn from (0.95,1.05)). If this is too much,
!the attempt is most likely going to be wasted because the choice will be vastly
!suboptimal. If it is too little, then it may not be enough to break from the
!pull of the local optimum at which we were previously stuck. 0.05 seems a reasonable
!starting point.
VFI_perm = 0.05

!If VFI_PSO = .true., then we use PSO in value function iteration to look for global
!optima every time that the number of attempts that should be done at a gridpoint
!is greater than 1 (this is usually the first iteration of VFI in the first iteration
!of CTS everywhere in the state space, and in later iterations of VFI this is the case in
!the 'box' given by VFIperm parameters given below. If VFI_PSO_only = .true., then
!the PSO is done but afterwards only 1 attempt at local improvement is made (without
!permutation). I.e., either PSO or the other permutation is done, never both.
!Furthremore, the rule is modified in the way that if the number of
!attempts is greater than 1, PSO is used only with probability PSO_p. This allows us
!to use PSO once in a while only. The draw is done at each gridpoint - so if PSO_p < 1.0,
!then PSO will be done at some gps in any iteration of VFI but not all. 
!Also - in the first iteration, if VFI_PSO = .true. and the number of attempts is greater than 1
!(as set by VFI_attmax and the associated randomization) - then the probability to use PSO is 1.
!(It makes sense to look for global optimum everywhere once, and then every couple of iterations
!check that we are not stuck in a bad local optimum by calling PSO).
!
!It is recommended to use PSO_p = 1.0, and rather use the paramaters related to VFI_attmax_rnd
!(which allows randomisation) to determine how often PSO should be used (if PSO_p = 1, it is used
!whenever the number of attempts at a gridpoint is greater than 1. This makes a lot of sense particularly
!when we use Howard algorithm - we would then want to set VFI_attmax_p1 = 1.0, a small number of attempts
!(VFI_attmax = 2) perhaps, and PSO_p = 1 -> to make sure that in the BOX (defined below) we always
!use PSO (otherwise with Howard this could lead to instability)
!
!VFI_PSO_par determines the number of particles used (50 is default and
!recommended value)
!
!PSO is computationally expensive so if we are not careful the algorithm will complete
!very few iterations.
!VFI_PSO_perm is the range for generating a subspace over which PSO is used. Default
!value is 0.1.

VFI_PSO = .true. !default is .false. (it's expensive and I need to test whether it's worth it)
VFI_PSO_only = .true. !default is .true.
VFI_PSO_p = 1.0 !default is 1.0
VFI_PSO_par = 50 !default is 50
VFI_PSO_perm = 0.1 !default is 0.1


!The parameter VFI_attmax_rnd determines the way VFI is randomized. If this is 0 (the
!default value, then there is no randomization and every iteration, there are
!VFI_attmax attempts made. Later on I will introduce randomization. There will be
!two probabilities VFI_attmax_p1 and VFI_attmax_p2, and the chance that
!we perform VFI_attmax rather than 1 attempt will range (at every grid point)
!between those two. If VFI_attmax rnd = 1, then the chance will be uniform
!and equal to VFI_attmax_p1. If this is 2, then the chance that permutation will
!be made (attmax > 1) will be VFI_attmax_p1  at points given in BOX defined by 
!parameters VFI_perm_... below, and it will be VFI_attmax_p2 elsewhere.
!
!The idea is that we want to be extra sure that we are not stuck
!in a local optimum in the part of the state space that is visited more often.
!(again, ideally we would do lots of attempts everywhere and every iteration
!but we can't afford this, so we do it more often in the regions of SS where
!we are likely to be). A problem with this is that if we are extra-careful in a region
!of state space about not being stuck in a local optimum, we are biasing the algorithm
!in the favour of staying in that region (because the value there will be
!higher). With sufficiently long run time so there is enough exploration even
!at point where the probability is lower it should be fine but we need to be careful
!about this).
!
!If VFI_attmax_rnd = 3, this is the same as 2 but the probability will drop continuously
!from the middle of the BOX where it will be p1, to the border where it will be p2, and
!everywhere outside of the BOX it will be p2. This has not been implemented yet.
!
!Also, when we are using PSO, this is only ever used at points where attmax is greater
!than 1 during a VFI iteration. So the way we set up VFI_attmax_rnd and the probabilities,
!(and CTS_first_attmax) has important implications for runtime and performance.
VFI_attmax_rnd = 2

!(note - if we want to use PSO in later stages than first iteration of CTS - we need
!to set this to a greater value than 1 (PSO is used only if attmax_rnd is set to
!a value greater than one as described above).

!(We should set p1 >= p2)
VFI_attmax_p1 = 1.0
VFI_attmax_p2 = 0.0

!I need to document this more properly. The above description of VFI_attmax_rnd is wrong
!for cases of above 0.
!I start with case 2 - 'deterministic hamburger' and then go to 3 (stochastic hamburger).


!No time now to document it - I need to submit a computation on HPC in about an hour.
!p1 probability uniform in the hamburger box, p2 outside (set to zero) - for now I assume
!it's zero in the code itself - I might generalize this later to offer some chance
!of randomization.
!
!Here define the box.
VFIperm_rho_min = 0.972 %values very close to zero should be avoided, could lead to errors
VFIperm_rho_max = 1.028

!a_min and a_max are vector which contain the lower and upper bounds for government debt
!for both countries (marginal-utility adjusted).
VFIperm_a_min = -1.0 -1.0
VFIperm_a_max = 1.0 1.0

VFIperm_aex_min = -2.5
VFIperm_aex_max = 2.5

!if sim_debug is tru, then additional information is displayed during simulation
!(this is a lot of information so it is impractical to use unless we are only simulating
!a single series).
sim_debug = .true.

!If sim_debug_par = .true., then the program prints out the count of optimal choices
!found by each CPU. This should be roughly uniform across all images except for 1 which
!is treated a bit differently.
sim_debug_par = .true.

!If sim_debug is true, then sim_debug_const is added to the mid point of the grid.
!This is purely for debugging purposes - such as figuring out whether it is possible
!to stay in the middle of the grid (rougly) if we deviate from it due to small
!mistakes in the early simulation periods
sim_debug_const = 0.0

!If sim_interp = .true., then we use the policy function (if it is available) to
!get the initial guess in simulation (and then we attempt to improve on this
!guess if sim_attmax2 > 0). If the policy function is not available (file
!x_pol_all.out does not exist), then this option is ignored and the initial
!guess is the optimal plan in the last period. This is usually worse because
!at a different point in state space the optimum might be quite different, but sometimes
!it is not a problem. In particular, if we only require a small number of simulations,
!we do not need to use the policy function (which takes a lot of space), and we might
!be fine just using many random permutations of the optimal policy from the previous
!period. The same algorithm is uses if sim_interp = .false. The default option is .true.
sim_interp = .true.

!If sim_takebest == .true., then if sim_interp == .true.
!(and we are thus using policy function interpolation), the algorithm computes thereturn
!of both the last-period solution xc_last, and the interpolated policy x_interp. It then
!picks the better of the two guesses and proceeds with the optimization.
!(this new point will be the center of permutations rather than the last-period solution or
!the interpolated policy function).
!This is useful for robustness. In most cases (if the change in state is small between the periods)
!the last-period value is better than the interpolated policy (very slightly). But relying on the
!interpolated policy all the time is dangerous because if we make a bad mistake once, it will
!affect the solution badly for several periods and perhaps even lead to some divergence...
!Therefore a good combination of parameters (ideal in my opinion) is sim_interp == .true.,
!and sim_takebest = .true. This will result in a situation where the last-period solution is
!used more frequently but not when it leads to a bad outcome in a period.
sim_takebest = .true.

!NOTE: I recommend to use sim_careful rather than sim_takebest for robustness reasons.
!If sim_careful is true, then sim_takebest is not used (not the best of the two
!but both of the two options are explored).

!If sim_careful = .true., then the permutations in simulation are not done around
!one point but around several points. These are the last-period solution, the interpolated
!policy function (if sim_interp is used), the best value found by PSO (if sim_PSO),
!and the best point found in the first stage of randomization which is driven by sim_attmax).
!More options can be added in the future. The motivation is that unless sim_perm2 is quite large
!if we use only one of centers of randomization, we might miss the true optimum. And if we use a
!larger sim_perm, and we move the center of randomization once, we might be very unlucky
!and move it in such a way that the true optimum lies at the edge of the new randomization
!box (this can happen because PSO is a heuristic algorithm which is not quaranteed to find
!the local optimum, and also in the first stage of randomization, unless the number of attemtps
!is absolutely huge, we might get a point which is better than the previously best one
!so far, but the sim_perm box around the point might not contain the true optimum). The
!hope is that by having multiple randomization centers, the true optimum should be
!in at least one of them. It's essentially a robustness feature - but it might be computationally
!expensive, because in the worst case (when it's not needed because the true optimum lies
!close to one of the points) we just waste roughly 2/3 of the attempts in the second stage
!of randomization (attmax 2b). But actually it's not so bad - becuase in the cases
!when it is not needed (because PSO, and xc_last, and xc_interp are very similar) it makes
!almost no difference, and in the cases when it is needed (because these points are substantially
!different) we really should use it!
sim_careful = .true.

!sim_attmax1 is the maximum number of attempts in first period (the number of random initializations).
!sim_attmax2 is the same for all consecutive periods (this can't be quite so large as the
!former parameter because the runtime could get too high, particularly if we are trying to do
!many simulations). sim_maxiter is the maximum number of iterations in the optimization subroutines
!used in simulation. In first period, it is multiplied by 10 (we can afford a lot more time there),
!and sometimes the algorithm converges slowly.
!If sim_attmax2 = 0, and sim_interp = .true., then we effectively just use the policy function
!(and interpolation) and we do not try to improve on this. I recommend a value of 1 or more,
!but not much more (maybe 10-100 in combination with small interp). Sometimes the policy function
!might not attain a good value
sim_attmax1 = 100
sim_attmax2 = 1 !sim_attmax2 is redundant in the latest version - remove it later.
sim_attmax2a = 100
sim_attmax2b = 1000
sim_maxiter = 1000

!If sim_par_keepattmax is true, then in the parallel impelementation of simulation, every CPU
!does all the attempts (so in fact the number of attempts will be the number of CPUS times the
!respective attmax. If it is true then the runtime does not decrease with an increasing
!number of CPUs (but possibly accuracy increases), and the result depends quite heavily on the
!number of CPUs used. The default value is .false. in which the attempts are split between the
!CPUs so that the total number of attempts is approximately the same. The runtime decreases in
!this time as long as attmax are significantly far from zero (there is some rounding upwards,
!and PSO is performed once on every CPU if it is performed so the result is still affected
!by the number of CPUs but not so much, the effect is mainly decreasing runtime for a given
!accuracy)
sim_par_keepattmax = .false.

!The following parameters (optional, otherwise default is 0.25), set the permutations
!of initial guess in first period, and in consequent periods, as share of the original initial
!guess (which is one consistent with autarky SS in the first period, and with last-period
!solution in the consequent periods). In the first attempt, the original guess is used,
!and then permutations are used in the following attempts.
!The values of these parameters should always be between 0 and 1, and usually quite close to zero
!(permutations of 25 percentage points or more rarely yield an optimum)
sim_perm1 = 0.1
sim_perm2 = 0.05

!If this is true, then in the first-period the interpolation used is quadrilinear
!rather than one of the more sophisticated ones.
sim_FP_quadrilin = .false.

!If sim_PSO is true then particle swarm optimization is used (once) every period in a '4D rectangle'
!(the same part of state space from which we draw the permutations and is determined by sim_perm2).
!This sometimes improves the accuracy of the simulation (but even after PSO is used, we still
!do all the permutations as before - but now the initial point is the best one found by PSO if
!it was better than the initial guess from earlier - which is either the interpolated policy
!function or the last-period solution). In simulations it should not really matter - but it is
!also helpful as a test of performance of PSO in this setting (if it performs really well
!it might be worth implementing in value function iteration). The default value is false.
sim_PSO = .true.

!sim_PSO_par determines the number of particles used in PSO. The recommended value byt NAG is
!50 for 5 free variables. A higher number leads to longer runtime but perhaps better performance.
sim_PSO_par = 50

!if sim_PSO_par > 0, then this will be used to generate the subspace in which PSO is run
!It is otherwise used in the same way as sim_perm1 and sim_perm2.
sim_PSO_perm = 0.1 

!opt_subroutine determines which optimization subroutine is used everywhere in the program (LFFC,
!VFI, accuracy tests, simulation). See the source code for the exact meaning.
!At this stage, only value 1 is supported everywhere. Value 2 is supported for simulations only.
opt_subroutine = 1

!Parameters related to particular optimization subroutines (see documentation of NAG Fortran Library Mark 24
!for explanation). Parameters with prefix 'sim_' are values used in simulation (in general there we can
!afford a bit more accuracy/more iterations)
!1: e04jcf
rhoend = 0.000001
sim_rhoend = 0.000000000001 


!If rhoendadapt = .true., then rhoend is the baseline value, but it is adjusted downwards at gridpoint,
!depending on the change in policy functions at that grid point in the last iteration. The idea is
!that in some regions of state space such as near the 'corners' or close to violating some constraints
!the policy can change quite a bit between iterations, and it is wasteful to try to find a really accurate
!solution there (which would be the case if we decreased rhoend globally). On the other hand, in some
!regions of the state space the solution changes very little between iterations (usually towards the middle
!of the state space - and if we keep a global rhoend high (so the program does not run for too long),
!we forego further improvements in the regions of state space in which we tend to spend the most
!time in simulatiions. The adaptation is that rhoend at a gridpoint in any iteration of VFI will be the
!minum of the global rhoend set above, and the average change in policy (in levels) at that gridpoint
!in the last iteration, multiplied by 1/10. We could also define a floor so that rhoend never
!gets too low but this should not be necessary because of the maximum number of evaluations
!is not that high in VFI iterations.
rhoendadapt = .true.

!3: e04aba
e1 = 0.000000001
e2 = 0.0000000001 
sim_e1 = 0.000000001
sim_e2 = 0.0000000001 


!_______________________Parameters related to loss functions___________________________

loss1a = 100.0
loss2a = 1000.0
loss3a = 100000.0

loss_const = 0.0
!loss_const is added every time that the loss is > 0. The point is that small violations of constraints
!should then never be optimal (of course, afterwards we always need to check that the grid boundaries
!do not affect the solution). The issue is that there is a discontinuity introduced which might
!be problematic to deal with for pretty much any optimization subroutine that we use.
!It should be possible to achieve roughly the same effect if the coefficients on loss are sufficiently
!severe. The default and suggested value of loss_const is zero - but a small value such as 1-10
!depending on the preferences might prevent 'cheating' always, and not cause too many issues.
!Another issue with this is that the fixed penalization is applied only once, so a point at which one constraint
!is violated just a little bit is punished by the same fixed penalty as points where multiple
!constraints are violated (at least in this fixed element). It's probably really not a good idea
!to use this option.

!All of these are used in subroutine check_constr in mod_union.f90 (see the source code for details).
!
!We define them here as opposed to hard-coding because these could possibly affect the solution
!quite a bit, and we need to experiment with these - values of loss might change, but as long
!as simulations do not change (or change very little), we are fine. Particularly for actually
!simulated series - these parameters can affect the policy in some 'corners' of the grid quite a lot
!but as long as we do not get there, it should not matter much.
!The lower the number # in loss#a, the lower level of severity it represents. The most 'severe'
!constraints are those on consumption and labour being withing their limits.
!However, negative consumption and labour supply is handled separately (a large fixed penalty is applied),
!and no evaluation is done (utility is undefined for such choices, so it is pointless to attempt local
!improvements)
!loss3a is used only for labour supply exceeding the labour endowment (which is a less severe violation of the constraints).
!The second group are constraints on ratio of marginal utilities - These constraints follow from the choice of grids,
!and if we are far from grid, this could yield to severe underestimation of the utility cost.
!In general, the grid should be wide enough that this does not happen often. Also, to some extent
!it is a scaling issue because rho tends to fluctuate a lot less than asset holdings. 
!The least severe violations are those of constraints on a and aex.
!
!So far there is only one parameter for each severity because loss is a sum of violation
!of each individual constraint to the power of two (so it is convex and simple), and this is then
!multiplied by the appropriate loss function parameter.

!___________________Parameters related to randomization of initial guess_______________________________
!These should be tempered with only with care, the way they are used is defined in source code.
!Another set of parameters which affect the randomization of initial guess are the attmax parameters
!in the section of solution on accuracy tests. The parameters here determine the bounds of the intervals

!LFFC_rnd_a, LFFC_rnd_b, LFFC_rnd_c are small numbers (should be in the (0,1) interval). The closer to 1, the
!more randomization there is. They are used in randomization in subroutine LFFC, and deeper in subroutine
!LFFC_randomize (check the source code for details). They need to be chosen experimentally (too low - we;ll
!probably end up with the same solution which may be bad. too high-we might start at some non-sensical guess
!and converge to a different solution, but that might be very bad one).
LFFC_rnd_a = 0.3
LFFC_rnd_b = 0.2
!if we use the old version of LFFC (LFFC_use_new_guess == .false.) then LFFC_rnd_b should not be more
!than about 0.2. If we use the new version, it makes most sense to set LFFC_rnd_b to a negative value
!(then the kappa in the new algorithm will essentially be randomized in (0,1) interval).
LFFC_rnd_c = 0.3

!Particularly LFFC_rnd_b should be quite close to zero. If this is high then there are often quite
!large deviations from rho, which means that country 2 state M consumption may fluctuate wildly.
!And then rho in state M will also be possibly quite far out of bounds.

!A different set of parameters when optimising during VFI iteration. In particularly attmax can
!increase the accurracy of the solution, but it can be extremely time-consuming. It is not
!clear what the optimal tradeoff is between attmax


!End of parameter file (this should be kept here to avoid potential eof problems)
